{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Guia RÃ¡pido - Cluster Hadoop\n",
    "\n",
    "Este notebook fornece um guia interativo para configurar, testar e validar um cluster Apache Hadoop completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## InÃ­cio RÃ¡pido (5 minutos)\n",
    "\n",
    "### 1. Iniciar o Cluster\n",
    "\n",
    "Execute o comando abaixo para iniciar todos os containers do cluster Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "start-cluster",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wait",
   "metadata": {},
   "source": [
    "â³ **Aguarde ~30 segundos** para inicializaÃ§Ã£o completa dos serviÃ§os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"Aguardando inicializaÃ§Ã£o do cluster...\")\n",
    "time.sleep(30)\n",
    "print(\"âœ“ Cluster deve estar pronto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify",
   "metadata": {},
   "source": [
    "### 2. Verificar Status dos Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-containers",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker-compose ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "web-interfaces",
   "metadata": {},
   "source": [
    "### 3. Acessar Interfaces Web\n",
    "\n",
    "Abra os seguintes links no seu navegador:\n",
    "\n",
    "- **HDFS NameNode**: [http://localhost:9870](http://localhost:9870)\n",
    "- **YARN ResourceManager**: [http://localhost:8088](http://localhost:8088)\n",
    "- **JobHistory Server**: [http://localhost:19888](http://localhost:19888)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-cluster",
   "metadata": {},
   "source": [
    "### 4. Verificar Cluster Hadoop\n",
    "\n",
    "#### Verificar DataNodes (HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-datanodes",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker exec hadoop-master hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-yarn",
   "metadata": {},
   "source": [
    "#### Verificar NodeManagers (YARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-nodemanagers",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker exec hadoop-master yarn node -list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-output",
   "metadata": {},
   "source": [
    "âœ… **Resultado Esperado:** Deve mostrar 2 DataNodes e 2 NodeManagers ativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wordcount",
   "metadata": {},
   "source": [
    "### 5. Executar Exemplo WordCount\n",
    "\n",
    "#### Copiar arquivos para o container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copy-wordcount",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker cp examples/wordcount hadoop-master:/tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-wordcount",
   "metadata": {},
   "source": [
    "#### Executar job WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-wordcount",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker exec hadoop-master bash -c \"\n",
    "  cd /tmp/wordcount\n",
    "  chmod +x *.py *.sh\n",
    "  ./run_wordcount.sh\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "view-results",
   "metadata": {},
   "source": [
    "### 6. Ver Resultados\n",
    "\n",
    "#### Ver todos os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker exec hadoop-master hdfs dfs -cat /user/root/wordcount/output/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "top-words",
   "metadata": {},
   "source": [
    "#### Top 10 palavras mais frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-top10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker exec hadoop-master bash -c \"\n",
    "  hdfs dfs -cat /user/root/wordcount/output/part-* | sort -t$'\\t' -k2 -nr | head -10\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "components",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Componentes do Projeto\n",
    "\n",
    "Este projeto estÃ¡ dividido em trÃªs componentes principais, cada um abordando um aspecto diferente do Apache Hadoop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component1",
   "metadata": {},
   "source": [
    "### 1ï¸âƒ£ Montagem de um Cluster Hadoop BÃ¡sico (ConfiguraÃ§Ã£o BÃ¡sica)\n",
    "\n",
    "**Objetivo:** Configurar e executar um cluster Hadoop funcional com Docker.\n",
    "\n",
    "**LocalizaÃ§Ã£o:**\n",
    "- `docker-compose.yml` - OrquestraÃ§Ã£o dos containers\n",
    "- `hadoop-config/` - Arquivos de configuraÃ§Ã£o do Hadoop\n",
    "- `start-master.sh` e `start-worker.sh` - Scripts de inicializaÃ§Ã£o\n",
    "\n",
    "**O que foi implementado:**\n",
    "- âœ… 1 nÃ³ master (NameNode + ResourceManager + JobHistory)\n",
    "- âœ… 2 nÃ³s workers (DataNode + NodeManager)\n",
    "- âœ… Interfaces web de monitoramento (portas 9870, 8088, 19888)\n",
    "- âœ… HDFS com fator de replicaÃ§Ã£o 2\n",
    "- âœ… YARN configurado com 2GB por NodeManager\n",
    "- âœ… MapReduce com JobHistory Server\n",
    "\n",
    "**DocumentaÃ§Ã£o:** Ver `README.md` para detalhes completos da arquitetura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component1-verify",
   "metadata": {},
   "source": [
    "#### Verificar configuraÃ§Ã£o do cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "component1-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"=== Status do Cluster ===\"\n",
    "docker-compose ps\n",
    "\n",
    "echo -e \"\\n=== DataNodes (HDFS) ===\"\n",
    "docker exec hadoop-master hdfs dfsadmin -report | grep -A 2 \"Live datanodes\"\n",
    "\n",
    "echo -e \"\\n=== NodeManagers (YARN) ===\"\n",
    "docker exec hadoop-master yarn node -list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Teste de Comportamento do Framework Hadoop\n",
    "\n",
    "**Objetivo:** Demonstrar como diferentes configuraÃ§Ãµes impactam performance e comportamento do HDFS, YARN e MapReduce.\n",
    "\n",
    "**LocalizaÃ§Ã£o:** `tests/`\n",
    "\n",
    "**5 Testes Implementados:**\n",
    "\n",
    "1. **test1_replication.sh** - Fator de replicaÃ§Ã£o HDFS (1, 2, 3)\n",
    "   - Impacto no uso de disco\n",
    "   - DistribuiÃ§Ã£o de blocos entre DataNodes\n",
    "   - Trade-off entre seguranÃ§a e espaÃ§o\n",
    "\n",
    "2. **test2_yarn_memory.sh** - MemÃ³ria YARN (1GB, 2GB, 4GB)\n",
    "   - NÃºmero de containers simultÃ¢neos\n",
    "   - Performance de jobs\n",
    "   - UtilizaÃ§Ã£o de recursos\n",
    "\n",
    "3. **test3_scheduler_queues.sh** - Filas do Capacity Scheduler\n",
    "   - Single queue vs multiple queues (high/default/low)\n",
    "   - PriorizaÃ§Ã£o de jobs\n",
    "   - Isolamento de recursos\n",
    "\n",
    "4. **test4_block_size.sh** - Tamanho de blocos HDFS (64MB, 128MB, 256MB)\n",
    "   - NÃºmero de map tasks geradas\n",
    "   - Overhead de metadados no NameNode\n",
    "   - Performance de I/O\n",
    "\n",
    "5. **test5_mapreduce_memory.sh** - MemÃ³ria de containers MapReduce\n",
    "   - MemÃ³ria para mappers e reducers (256MB, 512MB, 1024MB)\n",
    "   - Paralelismo vs consumo de recursos\n",
    "   - OtimizaÃ§Ã£o de performance\n",
    "\n",
    "**Resultados:** Arquivos salvos em `tests/results/` com mÃ©tricas detalhadas e anÃ¡lise comparativa.\n",
    "\n",
    "**DocumentaÃ§Ã£o:** Ver `tests/README.md` e `tests/TESTING_GUIDE.md` para detalhes de cada teste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component2-run-single",
   "metadata": {},
   "source": [
    "#### Executar teste individual (Teste 1: ReplicaÃ§Ã£o HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-test1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./tests/scripts/test1_replication.sh baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component2-run-all",
   "metadata": {},
   "source": [
    "#### Executar todos os testes (~35-40 minutos)\n",
    "\n",
    "âš ï¸ **Aviso:** Isso levarÃ¡ bastante tempo. Execute apenas se tiver tempo disponÃ­vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Descomente a linha abaixo para executar todos os testes\n",
    "# ./tests/run_all_tests.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component2-report",
   "metadata": {},
   "source": [
    "#### Gerar relatÃ³rio consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-report-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./tests/generate_report.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Teste de TolerÃ¢ncia a Falhas e Performance\n",
    "\n",
    "**Objetivo:** Avaliar resiliÃªncia do Hadoop sob condiÃ§Ãµes adversas e medir capacidade de recuperaÃ§Ã£o.\n",
    "\n",
    "**LocalizaÃ§Ã£o:** `fault-tolerance/`\n",
    "\n",
    "**4 CenÃ¡rios de Teste:**\n",
    "\n",
    "1. **Baseline** - Performance sem falhas\n",
    "   - Cluster completo (2 workers)\n",
    "   - ExecuÃ§Ã£o normal de WordCount\n",
    "   - Estabelece linha de base de tempo (~3-4 min)\n",
    "\n",
    "2. **Worker Failure** - Falha de 1 worker durante execuÃ§Ã£o\n",
    "   - Remove hadoop-worker1 apÃ³s 30s\n",
    "   - Testa recuperaÃ§Ã£o automÃ¡tica do YARN\n",
    "   - Job deve completar com ~20-40% mais tempo\n",
    "\n",
    "3. **Scale Up** - AdiÃ§Ã£o dinÃ¢mica de worker\n",
    "   - Inicia com 1 worker apenas\n",
    "   - Adiciona worker2 apÃ³s 30s\n",
    "   - Demonstra elasticidade do cluster\n",
    "\n",
    "4. **Multiple Failures** - Falhas mÃºltiplas (catastrÃ³fico)\n",
    "   - Remove ambos workers progressivamente\n",
    "   - Job deve FALHAR\n",
    "   - Identifica limites de tolerÃ¢ncia\n",
    "\n",
    "**Scripts disponÃ­veis:**\n",
    "- `generate_data.sh` - Gera dataset de 500MB+ para jobs longos\n",
    "- `upload_data.sh` - Upload para HDFS com verificaÃ§Ã£o\n",
    "- `monitor_job.sh` - Monitora jobs em tempo real\n",
    "- `run_fault_test.sh` - Orquestra os 4 testes\n",
    "- `generate_report.sh` - RelatÃ³rio consolidado com anÃ¡lise\n",
    "\n",
    "**MÃ©tricas coletadas:**\n",
    "- Tempo de execuÃ§Ã£o e recuperaÃ§Ã£o\n",
    "- Taxa de sucesso/falha\n",
    "- Comportamento do ResourceManager\n",
    "- Logs detalhados de cada cenÃ¡rio\n",
    "\n",
    "**DocumentaÃ§Ã£o:** Ver `fault-tolerance/README.md` para anÃ¡lise completa dos resultados esperados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component3-step1",
   "metadata": {},
   "source": [
    "#### Passo 1: Gerar dados de teste (500MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./fault-tolerance/scripts/generate_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component3-step2",
   "metadata": {},
   "source": [
    "#### Passo 2: Upload para HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./fault-tolerance/scripts/upload_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component3-step3",
   "metadata": {},
   "source": [
    "#### Passo 3a: Executar teste baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./fault-tolerance/scripts/run_fault_test.sh baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component3-step3b",
   "metadata": {},
   "source": [
    "#### Passo 3b: Executar teste de falha de worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-worker-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./fault-tolerance/scripts/run_fault_test.sh worker-failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component3-step3c",
   "metadata": {},
   "source": [
    "#### Passo 3c: Executar teste de scale up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-scaleup",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./fault-tolerance/scripts/run_fault_test.sh scale-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component3-step3d",
   "metadata": {},
   "source": [
    "#### Passo 3d: Executar teste de falhas mÃºltiplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-multiple-failures",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./fault-tolerance/scripts/run_fault_test.sh multiple-failures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component3-all",
   "metadata": {},
   "source": [
    "#### Ou executar todos os testes de tolerÃ¢ncia a falhas\n",
    "\n",
    "âš ï¸ **Aviso:** Isso levarÃ¡ aproximadamente 15-20 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all-fault-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Descomente a linha abaixo para executar todos os testes\n",
    "# ./fault-tolerance/scripts/run_fault_test.sh all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "component3-report",
   "metadata": {},
   "source": [
    "#### Passo 4: Gerar relatÃ³rio de tolerÃ¢ncia a falhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-fault-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./fault-tolerance/scripts/generate_report.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workflow",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Fluxo Recomendado de ExecuÃ§Ã£o\n",
    "\n",
    "Para executar o projeto completo na ordem correta:\n",
    "\n",
    "**Tempo total estimado:** ~50-60 minutos para todos os testes\n",
    "\n",
    "### Passo 1: Montar cluster bÃ¡sico (âœ… JÃ¡ executado acima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "workflow-step1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker-compose up -d\n",
    "sleep 30\n",
    "docker exec hadoop-master hdfs dfsadmin -report | grep -A 2 \"Live datanodes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workflow-step2",
   "metadata": {},
   "source": [
    "### Passo 2: Executar testes de comportamento (~35-40 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "workflow-step2-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Descomente para executar\n",
    "# ./tests/run_all_tests.sh\n",
    "# ./tests/generate_report.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workflow-step3",
   "metadata": {},
   "source": [
    "### Passo 3: Testes de tolerÃ¢ncia a falhas (~15-20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "workflow-step3-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Descomente para executar\n",
    "# ./fault-tolerance/scripts/generate_data.sh\n",
    "# ./fault-tolerance/scripts/upload_data.sh\n",
    "# ./fault-tolerance/scripts/run_fault_test.sh all\n",
    "# ./fault-tolerance/scripts/generate_report.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-commands",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comandos Ãšteis\n",
    "\n",
    "### Gerenciar Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Iniciar\n",
    "docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-stop",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Parar\n",
    "docker-compose down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-logs",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Ver logs\n",
    "docker logs hadoop-master --tail 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdfs-commands",
   "metadata": {},
   "source": [
    "### Trabalhar com HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdfs-mkdir",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Criar diretÃ³rio\n",
    "docker exec hadoop-master hdfs dfs -mkdir -p /user/root/meudir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdfs-ls",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Listar arquivos\n",
    "docker exec hadoop-master hdfs dfs -ls /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdfs-put",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Upload arquivo (exemplo)\n",
    "docker exec hadoop-master bash -c \"echo 'teste' > /tmp/teste.txt\"\n",
    "docker exec hadoop-master hdfs dfs -put /tmp/teste.txt /user/root/meudir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdfs-cat",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Ver conteÃºdo\n",
    "docker exec hadoop-master hdfs dfs -cat /user/root/meudir/teste.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yarn-commands",
   "metadata": {},
   "source": [
    "### Monitorar Jobs YARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yarn-list",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Listar aplicaÃ§Ãµes em execuÃ§Ã£o\n",
    "docker exec hadoop-master yarn application -list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yarn-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Status de uma aplicaÃ§Ã£o (substitua APPLICATION_ID)\n",
    "# docker exec hadoop-master yarn application -status <APPLICATION_ID>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structure",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Estrutura do Projeto\n",
    "\n",
    "```\n",
    "trabalho_hadoop/\n",
    "â”œâ”€â”€ docker-compose.yml              # ConfiguraÃ§Ã£o do cluster\n",
    "â”œâ”€â”€ hadoop-config/                  # Arquivos de configuraÃ§Ã£o\n",
    "â”‚   â”œâ”€â”€ core-site.xml              # ConfiguraÃ§Ãµes gerais\n",
    "â”‚   â”œâ”€â”€ hdfs-site.xml              # HDFS\n",
    "â”‚   â”œâ”€â”€ yarn-site.xml              # YARN\n",
    "â”‚   â”œâ”€â”€ mapred-site.xml            # MapReduce\n",
    "â”‚   â”œâ”€â”€ capacity-scheduler.xml     # Scheduler\n",
    "â”‚   â””â”€â”€ workers                    # Lista de workers\n",
    "â”œâ”€â”€ examples/\n",
    "â”‚   â””â”€â”€ wordcount/                 # Exemplo WordCount\n",
    "â”œâ”€â”€ tests/                         # Testes de comportamento\n",
    "â”‚   â”œâ”€â”€ scripts/                   # 5 scripts de teste\n",
    "â”‚   â”œâ”€â”€ results/                   # Resultados\n",
    "â”‚   â”œâ”€â”€ README.md\n",
    "â”‚   â””â”€â”€ TESTING_GUIDE.md\n",
    "â”œâ”€â”€ fault-tolerance/               # Testes de tolerÃ¢ncia a falhas\n",
    "â”‚   â”œâ”€â”€ scripts/                   # Scripts de teste\n",
    "â”‚   â”œâ”€â”€ data/                      # Dados gerados\n",
    "â”‚   â”œâ”€â”€ results/                   # Resultados\n",
    "â”‚   â””â”€â”€ README.md\n",
    "â”œâ”€â”€ README.md                       # DocumentaÃ§Ã£o completa\n",
    "â”œâ”€â”€ QUICK_START.md                 # Guia rÃ¡pido (texto)\n",
    "â””â”€â”€ Hadoop_Quick_Start.ipynb       # Este notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "## Recursos do Cluster\n",
    "\n",
    "- **NÃ³s**: 1 master + 2 workers\n",
    "- **MemÃ³ria YARN**: 4 GB total (2 GB/worker)\n",
    "- **CPUs**: 4 vCPUs (2/worker)\n",
    "- **ReplicaÃ§Ã£o HDFS**: 2 rÃ©plicas\n",
    "- **Hadoop Version**: 3.3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting",
   "metadata": {},
   "source": [
    "## SoluÃ§Ã£o RÃ¡pida de Problemas\n",
    "\n",
    "### DataNodes nÃ£o conectam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshoot-restart",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Reiniciar cluster\n",
    "docker-compose restart\n",
    "sleep 30\n",
    "\n",
    "# Ver logs\n",
    "docker logs hadoop-master --tail 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshoot-clean",
   "metadata": {},
   "source": [
    "### Limpar tudo e recomeÃ§ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Parar e remover volumes\n",
    "docker-compose down -v\n",
    "\n",
    "# Reiniciar\n",
    "docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## PrÃ³ximos Passos\n",
    "\n",
    "1. âœ… Cluster funcionando\n",
    "2. âœ… Exemplo WordCount executado\n",
    "3. ğŸ“ Executar testes de comportamento\n",
    "4. ğŸ“ Executar testes de tolerÃ¢ncia a falhas\n",
    "5. ğŸ“ Analisar relatÃ³rios gerados\n",
    "6. ğŸ“ Tirar screenshots das interfaces web\n",
    "7. ğŸ“ Documentar resultados para entrega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delivery",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Entrega do Trabalho\n",
    "\n",
    "Para documentar sua entrega, inclua:\n",
    "\n",
    "1. âœ… Arquivos de configuraÃ§Ã£o (`hadoop-config/`)\n",
    "2. âœ… Docker Compose configurado\n",
    "3. âœ… Screenshots das interfaces web\n",
    "4. âœ… Exemplo de job executado\n",
    "5. âœ… Resultados dos testes\n",
    "6. âœ… RelatÃ³rios gerados\n",
    "\n",
    "Tire screenshots de:\n",
    "- http://localhost:9870 (HDFS com 2 DataNodes)\n",
    "- http://localhost:8088 (YARN com jobs executados)\n",
    "- http://localhost:19888 (JobHistory)\n",
    "- Resultados dos testes de comportamento\n",
    "- Resultados dos testes de tolerÃ¢ncia a falhas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ ConclusÃ£o\n",
    "\n",
    "VocÃª agora tem:\n",
    "- âœ… Um cluster Hadoop completo e funcional\n",
    "- âœ… Framework de testes de comportamento (5 testes)\n",
    "- âœ… Framework de testes de tolerÃ¢ncia a falhas (4 cenÃ¡rios)\n",
    "- âœ… DocumentaÃ§Ã£o completa e interativa\n",
    "\n",
    "Execute as cÃ©lulas acima na ordem para configurar e testar seu cluster!\n",
    "\n",
    "**Boa sorte! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
