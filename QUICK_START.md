# Guia RÃ¡pido - Cluster Hadoop

## InÃ­cio RÃ¡pido (5 minutos)

### 1. Iniciar o Cluster

```bash
docker-compose up -d
```

Aguarde ~30 segundos para inicializaÃ§Ã£o completa.

### 2. Verificar Status

```bash
docker-compose ps
```

Todos os 3 containers devem estar "Up".

### 3. Acessar Interfaces Web

- **HDFS NameNode**: http://localhost:9870
- **YARN ResourceManager**: http://localhost:8088
- **JobHistory Server**: http://localhost:19888

### 4. Verificar Cluster

```bash
# Ver DataNodes conectados
docker exec hadoop-master hdfs dfsadmin -report

# Ver NodeManagers ativos
docker exec hadoop-master yarn node -list
```

Deve mostrar 2 DataNodes e 2 NodeManagers.

### 5. Executar Exemplo WordCount

```bash
# Copiar arquivos para container
docker cp examples/wordcount hadoop-master:/tmp/

# Executar job
docker exec hadoop-master bash -c "
  cd /tmp/wordcount
  chmod +x *.py *.sh
  ./run_wordcount.sh
"
```

### 6. Ver Resultados

O script mostrarÃ¡ os resultados automaticamente. VocÃª tambÃ©m pode:

```bash
# Ver resultados manualmente
docker exec hadoop-master hdfs dfs -cat /user/root/wordcount/output/part-*

# Top 10 palavras
docker exec hadoop-master bash -c "
  hdfs dfs -cat /user/root/wordcount/output/part-* | sort -t$'\t' -k2 -nr | head -10
"
```

## Comandos Ãšteis

### Gerenciar Cluster

```bash
# Iniciar
docker-compose up -d

# Parar
docker-compose down

# Parar e remover dados
docker-compose down -v

# Ver logs
docker logs hadoop-master
docker logs hadoop-worker1

# Reiniciar
docker-compose restart
```

### Trabalhar com HDFS

```bash
# Acessar container master
docker exec -it hadoop-master bash

# Criar diretÃ³rio
hdfs dfs -mkdir -p /user/root/meudir

# Upload arquivo
hdfs dfs -put arquivo.txt /user/root/meudir/

# Listar arquivos
hdfs dfs -ls /user/root/

# Ver conteÃºdo
hdfs dfs -cat /user/root/meudir/arquivo.txt

# Download arquivo
hdfs dfs -get /user/root/meudir/arquivo.txt ./

# Remover arquivo
hdfs dfs -rm /user/root/meudir/arquivo.txt

# Remover diretÃ³rio
hdfs dfs -rm -r /user/root/meudir
```

### Executar Jobs MapReduce

```bash
# Job WordCount built-in
docker exec hadoop-master bash -c "
  echo 'hello world hello hadoop' > /tmp/input.txt
  hdfs dfs -mkdir -p /input
  hdfs dfs -put /tmp/input.txt /input/
  hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
    wordcount /input /output
  hdfs dfs -cat /output/part-*
"
```

### Monitorar Jobs

```bash
# Listar aplicaÃ§Ãµes YARN
docker exec hadoop-master yarn application -list

# Status de uma aplicaÃ§Ã£o
docker exec hadoop-master yarn application -status <APPLICATION_ID>

# Ver logs de aplicaÃ§Ã£o
docker exec hadoop-master yarn logs -applicationId <APPLICATION_ID>
```

## Estrutura do Projeto

```
trabalho_hadoop/
â”œâ”€â”€ docker-compose.yml              # ConfiguraÃ§Ã£o do cluster
â”œâ”€â”€ hadoop-config/                  # Arquivos de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ core-site.xml              # ConfiguraÃ§Ãµes gerais
â”‚   â”œâ”€â”€ hdfs-site.xml              # HDFS
â”‚   â”œâ”€â”€ yarn-site.xml              # YARN
â”‚   â”œâ”€â”€ mapred-site.xml            # MapReduce
â”‚   â””â”€â”€ workers                    # Lista de workers
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ wordcount/                 # Exemplo WordCount
â”‚       â”œâ”€â”€ mapper.py
â”‚       â”œâ”€â”€ reducer.py
â”‚       â”œâ”€â”€ run_wordcount.sh
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ README.md                       # DocumentaÃ§Ã£o completa
â””â”€â”€ QUICK_START.md                 # Este guia
```

## Recursos do Cluster

- **NÃ³s**: 1 master + 2 workers
- **MemÃ³ria YARN**: 4 GB total (2 GB/worker)
- **CPUs**: 4 vCPUs (2/worker)
- **ReplicaÃ§Ã£o HDFS**: 2 rÃ©plicas

## SoluÃ§Ã£o RÃ¡pida de Problemas

### DataNodes nÃ£o conectam

```bash
# Reiniciar cluster
docker-compose restart

# Ver logs
docker logs hadoop-master
docker logs hadoop-worker1
```

### Job falha

1. Acesse http://localhost:8088
2. Clique no job
3. Veja logs nos containers

### Limpar tudo

```bash
# Parar e remover volumes
docker-compose down -v

# Limpar sistema Docker
docker system prune -a --volumes

# Reiniciar
docker-compose up -d
```

## PrÃ³ximos Passos

1. âœ… Cluster funcionando
2. âœ… Exemplo WordCount executado
3. ğŸ“ Criar seu prÃ³prio job MapReduce
4. ğŸ“ Processar datasets maiores
5. ğŸ“ Experimentar com mÃºltiplos reducers
6. ğŸ“ Implementar outros algoritmos (sorting, join, etc.)

## Recursos Adicionais

- **README.md**: DocumentaÃ§Ã£o completa
- **examples/wordcount/README.md**: Detalhes do exemplo
- Interfaces web para monitoramento
- Logs em tempo real via `docker logs`

## Entrega do Trabalho

Para documentar sua entrega, inclua:

1. âœ… Arquivos de configuraÃ§Ã£o (`hadoop-config/`)
2. âœ… Docker Compose configurado
3. âœ… Screenshots das interfaces web
4. âœ… Exemplo de job executado
5. âœ… DocumentaÃ§Ã£o dos arquivos

Tire screenshots de:
- http://localhost:9870 (HDFS com 2 DataNodes)
- http://localhost:8088 (YARN com job executado)
- http://localhost:19888 (JobHistory)
- SaÃ­da do comando `hdfs dfsadmin -report`
- Resultado do WordCount

Boa sorte! ğŸš€
